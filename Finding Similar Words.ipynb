{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Finding Similar Words.ipynb","provenance":[],"authorship_tag":"ABX9TyMmFjwHAutOUzGCccXpebG4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AL6WAJDCVi9x","executionInfo":{"status":"ok","timestamp":1611375495120,"user_tz":180,"elapsed":5014,"user":{"displayName":"Alessandra Faria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEz7l0NE5EQzujg-9Kkdl7EsZC4MCog5QGFlvZqQ=s64","userId":"18200841237099022086"}},"outputId":"3746f63c-c693-446c-b6d2-dac45884083b"},"source":["pip install textdistance"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting textdistance\n","  Downloading https://files.pythonhosted.org/packages/35/71/87133323736b9b0180f600d477507318dae0abde613a54df33bfd0248614/textdistance-4.2.0-py3-none-any.whl\n","Installing collected packages: textdistance\n","Successfully installed textdistance-4.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"177Db3NQXCLQ","executionInfo":{"status":"ok","timestamp":1611375507806,"user_tz":180,"elapsed":1675,"user":{"displayName":"Alessandra Faria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEz7l0NE5EQzujg-9Kkdl7EsZC4MCog5QGFlvZqQ=s64","userId":"18200841237099022086"}}},"source":["import pandas_profiling\r\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMQHvgy5Td2p","executionInfo":{"status":"ok","timestamp":1611375511148,"user_tz":180,"elapsed":1279,"user":{"displayName":"Alessandra Faria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEz7l0NE5EQzujg-9Kkdl7EsZC4MCog5QGFlvZqQ=s64","userId":"18200841237099022086"}}},"source":["import pandas as pd\r\n","import numpy as np\r\n","import textdistance\r\n","import re\r\n","from collections import Counter"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sjefo0rYnDfB","executionInfo":{"status":"ok","timestamp":1611375801332,"user_tz":180,"elapsed":26994,"user":{"displayName":"Alessandra Faria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEz7l0NE5EQzujg-9Kkdl7EsZC4MCog5QGFlvZqQ=s64","userId":"18200841237099022086"}},"outputId":"9e031440-31fb-4402-9cc2-75c3e84cc264"},"source":["!pip install -U -q PyDrive\r\n","from pydrive.auth import GoogleAuth\r\n","from pydrive.drive import GoogleDrive\r\n","from google.colab import auth\r\n","from oauth2client.client import GoogleCredentials\r\n","# Authenticate and create the PyDrive client.\r\n","auth.authenticate_user()\r\n","gauth = GoogleAuth()\r\n","gauth.credentials = GoogleCredentials.get_application_default()\r\n","drive = GoogleDrive(gauth)\r\n","link =  \"https://drive.google.com/open?id=1ziJ_81tOLm_Krclc5sBxKPJ1yc4wTQzo\" #LINK DA BASE ORIGINAL\r\n","#\"https://drive.google.com/file/d/10Un2f-CEL3D4oPZbP6nlJhtt8wAnUHn-/view?usp=sharing\" #LINK DA BASE USADA NO TCC\r\n","fluff, id = link.split('=')\r\n","print (id) # Verify that you have everything after '='\r\n","downloaded = drive.CreateFile({'id':id}) \r\n","downloaded.GetContentFile('book.txt')  \r\n","\r\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["1ziJ_81tOLm_Krclc5sBxKPJ1yc4wTQzo\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jtSFMjVoJQZ","executionInfo":{"status":"ok","timestamp":1611375803777,"user_tz":180,"elapsed":966,"user":{"displayName":"Alessandra Faria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEz7l0NE5EQzujg-9Kkdl7EsZC4MCog5QGFlvZqQ=s64","userId":"18200841237099022086"}},"outputId":"08ffaf70-f266-4e1f-d7a4-7caabf3a24c1"},"source":["words = []\r\n","with open('book.txt', 'r') as f:\r\n","    file_name_data = f.read()\r\n","    file_name_data=file_name_data.lower()\r\n","    words = re.findall('\\w+',file_name_data)\r\n","# This is our vocabulary\r\n","V = set(words)\r\n","print(f\"The first ten words in the text are: \\n{words[0:10]}\")\r\n","print(f\"There are {len(V)} unique words in the vocabulary.\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["The first ten words in the text are: \n","['the', 'project', 'gutenberg', 'ebook', 'of', 'moby', 'dick', 'or', 'the', 'whale']\n","There are 17647 unique words in the vocabulary.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YXZhEwMhXOv6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_eLAI0fSonBw","executionInfo":{"status":"ok","timestamp":1611375806468,"user_tz":180,"elapsed":865,"user":{"displayName":"Alessandra Faria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEz7l0NE5EQzujg-9Kkdl7EsZC4MCog5QGFlvZqQ=s64","userId":"18200841237099022086"}},"outputId":"9dcf2aee-3d9b-4596-f5ff-237c7ac037f1"},"source":["word_freq_dict = {}  \r\n","word_freq_dict = Counter(words)\r\n","print(word_freq_dict.most_common()[0:30])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[('the', 14703), ('of', 6742), ('and', 6517), ('a', 4799), ('to', 4707), ('in', 4238), ('that', 3081), ('it', 2534), ('his', 2530), ('i', 2120), ('he', 1896), ('but', 1823), ('s', 1819), ('with', 1769), ('as', 1752), ('is', 1751), ('was', 1646), ('for', 1644), ('all', 1544), ('this', 1439), ('at', 1335), ('whale', 1230), ('by', 1222), ('not', 1173), ('from', 1104), ('on', 1073), ('so', 1066), ('him', 1065), ('be', 1063), ('you', 958)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"c6TNPoYCXA1w","executionInfo":{"status":"ok","timestamp":1611375809186,"user_tz":180,"elapsed":1006,"user":{"displayName":"Alessandra Faria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEz7l0NE5EQzujg-9Kkdl7EsZC4MCog5QGFlvZqQ=s64","userId":"18200841237099022086"}},"outputId":"5add8859-2475-4c79-d432-327762e71288"},"source":["df = pd.DataFrame(words, columns=['Words'])\r\n","df.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>project</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>gutenberg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ebook</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>of</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Words\n","0        the\n","1    project\n","2  gutenberg\n","3      ebook\n","4         of"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"dXVQe7K4otjg","executionInfo":{"status":"ok","timestamp":1611375811859,"user_tz":180,"elapsed":770,"user":{"displayName":"Alessandra Faria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEz7l0NE5EQzujg-9Kkdl7EsZC4MCog5QGFlvZqQ=s64","userId":"18200841237099022086"}}},"source":["probs = {}     \r\n","Total = sum(word_freq_dict.values())    \r\n","for k in word_freq_dict.keys():\r\n","    probs[k] = word_freq_dict[k]/Total"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRACd8nzo224","executionInfo":{"status":"ok","timestamp":1611375888758,"user_tz":180,"elapsed":858,"user":{"displayName":"Alessandra Faria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEz7l0NE5EQzujg-9Kkdl7EsZC4MCog5QGFlvZqQ=s64","userId":"18200841237099022086"}}},"source":["def my_autocorrect(input_word):\r\n","    input_word = input_word.lower()\r\n","    if input_word in V:\r\n","      return 'Your word seems to be correct'\r\n","    else:\r\n","        similarities = [1-(textdistance.Jaccard(qval=2).distance(v,input_word)) for v in word_freq_dict.keys()]\r\n","        \r\n","        df = pd.DataFrame.from_dict(probs, orient='index').reset_index()\r\n","        df = df.rename(columns={'index':'Word', 0:'Prob'})\r\n","        df['Similarity'] = similarities\r\n","        output = df.sort_values(['Similarity', 'Prob'], ascending=False).head()\r\n","        return(output)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"omISMGl3pS3f","executionInfo":{"status":"ok","timestamp":1611377093975,"user_tz":180,"elapsed":1620,"user":{"displayName":"Alessandra Faria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEz7l0NE5EQzujg-9Kkdl7EsZC4MCog5QGFlvZqQ=s64","userId":"18200841237099022086"}},"outputId":"cb4ad145-f8d3-4229-8f82-4f59bff24182"},"source":["my_autocorrect('nelvilie')"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Word</th>\n","      <th>Prob</th>\n","      <th>Similarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>11</th>\n","      <td>melville</td>\n","      <td>0.000018</td>\n","      <td>0.400000</td>\n","    </tr>\n","    <tr>\n","      <th>5797</th>\n","      <td>belie</td>\n","      <td>0.000009</td>\n","      <td>0.375000</td>\n","    </tr>\n","    <tr>\n","      <th>9058</th>\n","      <td>relieving</td>\n","      <td>0.000004</td>\n","      <td>0.363636</td>\n","    </tr>\n","    <tr>\n","      <th>11703</th>\n","      <td>believing</td>\n","      <td>0.000004</td>\n","      <td>0.363636</td>\n","    </tr>\n","    <tr>\n","      <th>4541</th>\n","      <td>relief</td>\n","      <td>0.000027</td>\n","      <td>0.333333</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Word      Prob  Similarity\n","11      melville  0.000018    0.400000\n","5797       belie  0.000009    0.375000\n","9058   relieving  0.000004    0.363636\n","11703  believing  0.000004    0.363636\n","4541      relief  0.000027    0.333333"]},"metadata":{"tags":[]},"execution_count":35}]}]}